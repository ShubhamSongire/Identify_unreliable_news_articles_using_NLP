{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ladzQ9snHOUP"
   },
   "source": [
    "## FAKE NEWS CLASSIFIER WITH MACHINE LEARNING ALGORITHMS USING Natural Language Processing- PART 1\n",
    "\n",
    "kaggle dataset:- https://www.kaggle.com/c/fake-news/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data from kaggle competations without downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZXdXjLRHGNp",
    "outputId": "a7e8c7bc-6b8d-4a72-f980-b15552246428",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPD8K8LUHxaK"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle  #Make a directory named “.kaggle”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz3Ix79QH8ZB"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json \n",
    "              # Now, you need to copy the “kaggle.json” file from the mounted google drive to the current instance \n",
    "              #storage. \\The Google drive is mounted under the “./content/drive/MyDrive”  path. Just run the copy command as used in Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbxJpYErKSZd"
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7QtTDLhLJ_z",
    "outputId": "96b1cf79-3c48-4fea-e2be-64ce8116b1a5"
   },
   "outputs": [],
   "source": [
    "! kaggle competitions download -c fake-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBr59UXBMTPJ",
    "outputId": "6ae13aca-755f-48ba-c50c-714f709e2596"
   },
   "outputs": [],
   "source": [
    "!unzip /content/train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDpWGSskNGva",
    "outputId": "fb48c96c-c5f1-4dd1-b218-19cd8a29f938"
   },
   "outputs": [],
   "source": [
    "!unzip /content/test.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDpltxnjNJ6-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer ,HashingVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uZwXZsaANb00",
    "outputId": "2e3381f9-45ea-493a-d333-85f1f21f6337"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/content/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnfbR0AgQBs2",
    "outputId": "35bdecec-6027-46b5-f79c-735dde166b67"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows which are having blank columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klu5S6jSWyoB",
    "outputId": "585e85f7-7b9a-48ee-bc4e-b7f146c7c63e"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "gHnQ290xn7gJ",
    "outputId": "75ec1818-7448-4adf-b40c-a405c90bc9bd"
   },
   "outputs": [],
   "source": [
    "messages = df.copy() # here we are just resetting the indexes because of dropna 2000 rows are removed so.\n",
    "messages.reset_index(inplace=True)\n",
    "messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImaQ4_1KtpCf",
    "outputId": "3e6ede43-c9c2-4416-f9f6-250363a265ca"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJA-3S8Uo8C2",
    "outputId": "a26f627a-a907-4777-90c9-6de44fc8bf88"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "#x = 0\n",
    "for i in range(0,len(messages)):\n",
    "  #print(\"1 \", messages['title'][i])\n",
    "  review = re.sub('[^a-zA-Z]', \" \", messages['title'][i])\n",
    "  #print(\"2 \", review)  \n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  #print(\"4 \", review)  \n",
    "  review = [ps.stem(word) for word in review if not word  in stopwords.words('english')]\n",
    "  print(\"5 \", review)  \n",
    "  review = \" \".join(review)\n",
    "  corpus.append(review)\n",
    "  #x+=1\n",
    "  #if x==5:\n",
    "  #  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTCHsxlXq-qi",
    "outputId": "e01dd9c6-fc52-4307-d8d9-16a31f9dbd0d"
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FSO-3NZuUhv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating the bag of words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1,3) ) #1,3 che relational pairs banavat aahe ithe. means it will take 1 word then 2 word then 3 words like\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc_7emPBz63V",
    "outputId": "5762e7f4-92e6-42a1-9aa0-74e98eb13664"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dZo9m6q1Ql8",
    "outputId": "3904fc86-104b-4247-d116-a3dc734918c2"
   },
   "outputs": [],
   "source": [
    "y = messages['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the dataset into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSapS7X_1Ztu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPgtsNh92A00",
    "outputId": "0edfae6f-f293-4fe4-fedd-90fe680c28e7"
   },
   "outputs": [],
   "source": [
    "print(len(\"Total features created are:- \",cv.get_feature_names())) \n",
    "cv.get_feature_names()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zdu1MN879R4i",
    "outputId": "6c059db2-9dc0-4b41-bb57-cb8170a51df1"
   },
   "outputs": [],
   "source": [
    "cv.get_params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "mE4NwJ6v-DFy",
    "outputId": "3a68a7e0-def3-4a02-b492-5af79cc51cd8"
   },
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(X_train, columns=cv.get_feature_names()) #this means in NLP model get trained just like multiclass classification/regression problem.\n",
    "count_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the function to plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcDRoBej_hFE"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "   \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtedy-qBKPhG"
   },
   "source": [
    "### Passive Aggressive Classifier Algorithm\n",
    "this algorithm works well with text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGn6D5KwIObu"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "linear_clf = PassiveAggressiveClassifier(n_iter_no_change=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "OtAIpSEYJTxb",
    "outputId": "3045a78e-88e5-40b1-f65a-4ae301481b74"
   },
   "outputs": [],
   "source": [
    "linear_clf.fit(X_train, y_train) # here we got better accuracy than previous one\n",
    "pred = linear_clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RAVQ7GpLSXj"
   },
   "source": [
    "### Multinomial Classifier with Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9RYx69ILjS2"
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB(alpha=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiZ-ow3eRB0p",
    "outputId": "44c3b866-07d5-4bde-c609-5c9cdb71bcaa"
   },
   "outputs": [],
   "source": [
    "# here we will take values from 0 to 1 in step of 0.1\n",
    "score = 0\n",
    "for i in np.arange(0,1.1,0.1): #1.1 for to use last value as 1\n",
    "  #print(i)\n",
    "  classifier = MultinomialNB(alpha=i)\n",
    "  classifier.fit(X_train, y_train)\n",
    "  pred = classifier.predict(X_test)\n",
    "  new_score = metrics.accuracy_score(y_test, pred)\n",
    "  if new_score > score:\n",
    "    score = new_score\n",
    "  print(f\"score for {i} alpha is {new_score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tv_Zx7seRJl5",
    "outputId": "cee1b6b7-19c8-40b0-a77d-97005ef73432"
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "JbnZELi-RbNA",
    "outputId": "6836255e-65e9-42a2-9aaa-46a1405d11bc"
   },
   "outputs": [],
   "source": [
    "# so we will use alpha as 0.7 to get better results\n",
    "classifier = MultinomialNB(alpha=0.7)\n",
    "classifier.fit(X_train, y_train) \n",
    "pred = classifier.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %f\" % score)\n",
    "cm = metrics.confusion_matrix(y_test, pred)\n",
    "plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Otm8SlsdXJK1",
    "outputId": "9b8958a6-2657-4a15-bb36-cff18fbe6554",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Features names\n",
    "feature_names = cv.get_feature_names() # this are 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVqklcpWntZ",
    "outputId": "25162811-ef95-4178-fb88-ed2c17ef7fc1"
   },
   "outputs": [],
   "source": [
    "classifier.coef_[0] # this is array for all 5000 values. how much they are +ve or -ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieiZegEeWygw",
    "outputId": "8225b146-2d68-47c9-b5bd-aa4fcfdd2ebf"
   },
   "outputs": [],
   "source": [
    "# Most real top 20 words\n",
    "sorted(zip(classifier.coef_[0], feature_names), reverse=True)[:20] # here we have combined words with their accuracy and sorted it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "So9NCh4aXaA2",
    "outputId": "f0e25946-f6a4-488f-ff68-0ecc20a8c5d8"
   },
   "outputs": [],
   "source": [
    "# Most fake 20 words\n",
    "sorted(zip(classifier.coef_[0], feature_names))[:20] "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Vid-13 FAKE NEWS CLASSIFIER WITH MACHINE LEARNING ALGORITHMS USING Natural Language Processing- PART 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
